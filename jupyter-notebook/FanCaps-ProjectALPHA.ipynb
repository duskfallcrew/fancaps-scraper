{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd93b433-b301-49dd-a9a4-ae9480e4e422",
   "metadata": {},
   "source": [
    "![GitHub followers](https://img.shields.io/github/followers/duskfallcrew)  ![GitHub Sponsors](https://img.shields.io/github/sponsors/duskfallcrew) ![Static Badge](https://img.shields.io/badge/https%3A%2F%2Fgithub.com%2Fduskfallcrew%2Ffancaps-scraper%2F?style=flat-square&logo=github)\n",
    "\n",
    "Follow our discord: \n",
    "[Discord](https://discord.gg/5t2kYxt7An)\n",
    "\n",
    "# FanCaps-Scrapper\n",
    "\n",
    "A Jupyter notebook (not for colab) based on the Python CLI scrapper for anime screenshots & Fan Screen on https://fancaps.net.\n",
    "\n",
    "You will need an Ngork account for this, and this will NOT work on colab because of this. \n",
    "\n",
    "README template from https://www.makeareadme.com/\n",
    "\n",
    "This was forked from [Fannovel](https://github.com/Fannovel16/fancaps-scraper) , As well as the most recent fork for NodeJS: [JSarvise](https://github.com/JSarvise/fancaps-scraper)\n",
    "\n",
    "## About This Notebook\n",
    "\n",
    "Based on Fannovel's original Node JS application for mass downloading caps from FanCaps.\n",
    "This aims to try and make it as simple as possible, this one contains everything in house, and will continue to build on the original colab Fannovel developed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ac42bf-e273-474e-bec0-83144604c6c8",
   "metadata": {},
   "source": [
    "# Install Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0a7370-c36a-4393-bbb1-30228a221258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Function to install requirements from requirements.txt\n",
    "def install_requirements():\n",
    "    print(\"Installing requirements from requirements.txt...\")\n",
    "    try:\n",
    "        subprocess.run([\"pip\", \"install\", \"-r\", \"fancaps-scraper/jupyter-notebook/requirements.txt\"], check=True)\n",
    "        print(\"Requirements installed successfully.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error installing requirements: {e.stderr}\")\n",
    "\n",
    "# Markdown cell for explanation\n",
    "markdown_text = \"\"\"\n",
    "### Setting Up Environment\n",
    "\n",
    "To ensure all dependencies are met, we will proceed with installing the required packages \n",
    "from the `requirements.txt` file.\n",
    "\n",
    "#### Cloning Repositories\n",
    "\n",
    "Before we begin, let's clone the following repositories:\n",
    "- [fancaps-scraper](https://github.com/duskfallcrew/fancaps-scraper/)\n",
    "- [cafe-aesthetic-scorer](https://github.com/duskfallcrew/cafe-aesthetic-scorer)\n",
    "- [kohya-trainer](https://github.com/duskfallcrew/kohya-trainer)\n",
    "\n",
    "These repositories contain necessary components for running the notebook.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(markdown_text)\n",
    "\n",
    "# Clone the fancaps-scraper repository (if not already cloned)\n",
    "repo_urls = [\n",
    "    'https://github.com/duskfallcrew/fancaps-scraper/',\n",
    "    'https://github.com/duskfallcrew/cafe-aesthetic-scorer',\n",
    "    'https://github.com/duskfallcrew/kohya-trainer'\n",
    "]\n",
    "\n",
    "for repo_url in repo_urls:\n",
    "    clone_command = ['git', 'clone', repo_url]\n",
    "\n",
    "    # Execute the clone command using subprocess\n",
    "    try:\n",
    "        result = subprocess.run(clone_command, capture_output=True, text=True, check=True)\n",
    "        print(result.stdout)\n",
    "        print(f\"Repository {repo_url} cloned successfully.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error cloning repository {repo_url}: {e.stderr}\")\n",
    "\n",
    "# Install requirements using the function\n",
    "install_requirements()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cd611f-9235-49f1-b5c1-5332f97b0272",
   "metadata": {},
   "source": [
    "# Scraper CLI Wrapper with Ipython Widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7616719-ab31-4e3d-a4bd-5bb9b791aa9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6da1fb0954f549e791cfd0c0ba05b521",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='URL:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "941bfc0db58f4a8a8bbc1451768f5c1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Save Directory:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75e41dd6096f4d2fb3b9405954f774b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=75, description='Number of Promises:', min=1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4535b7d658384aa1815a77eec0721587",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=2, description='Skip Last Pages:', max=10)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3182d5eeb6b44e2eaed3e2d8fc15a435",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=False, description='Disable Progress Bar')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f82848ea0204b16a2d7d611b446bd77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Run Scraper', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import logging\n",
    "from urllib.parse import urlparse, urljoin\n",
    "import aiohttp\n",
    "from bs4 import BeautifulSoup\n",
    "import aiofiles\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Function to fetch HTML content using aiohttp\n",
    "async def fetch_html(session, url):\n",
    "    try:\n",
    "        async with session.get(url) as response:\n",
    "            return await response.text()\n",
    "    except aiohttp.ClientError as e:\n",
    "        logging.error(f\"Error fetching HTML from {url}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Function to download an image asynchronously\n",
    "async def download_image(session, image_url, save_dir):\n",
    "    try:\n",
    "        async with session.get(image_url) as response:\n",
    "            if response.status == 200:\n",
    "                image_data = await response.read()\n",
    "                filename = os.path.basename(urlparse(image_url).path)\n",
    "                save_path = os.path.join(save_dir, filename)\n",
    "                async with aiofiles.open(save_path, 'wb') as f:\n",
    "                    await f.write(image_data)\n",
    "                logging.info(f\"Downloaded: {image_url}\")\n",
    "            else:\n",
    "                logging.error(f\"Failed to download {image_url}. Status: {response.status}\")\n",
    "    except aiohttp.ClientError as e:\n",
    "        logging.error(f\"Error downloading {image_url}: {str(e)}\")\n",
    "\n",
    "# Function to download images asynchronously using aria2c\n",
    "async def download_images_async(image_urls, save_dir):\n",
    "    tasks = []\n",
    "    for image_url in image_urls:\n",
    "        tasks.append(download_image_async(image_url, save_dir))\n",
    "    await asyncio.gather(*tasks)\n",
    "\n",
    "async def download_image_async(image_url, save_dir):\n",
    "    try:\n",
    "        aria2c_command = f\"aria2c -x 16 -s 16 -d {save_dir} {image_url}\"\n",
    "        process = await asyncio.create_subprocess_shell(aria2c_command, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE)\n",
    "        stdout, stderr = await process.communicate()\n",
    "\n",
    "        if process.returncode == 0:\n",
    "            logging.info(f\"Downloaded: {image_url}\")\n",
    "        else:\n",
    "            logging.error(f\"Failed to download {image_url}. Error: {stderr.decode().strip()}\")\n",
    "    except asyncio.subprocess.SubprocessError as e:\n",
    "        logging.error(f\"Error running aria2c for {image_url}: {str(e)}\")\n",
    "\n",
    "# Function to validate the save directory\n",
    "def validate_save_dir(save_dir):\n",
    "    if not os.path.isabs(save_dir):\n",
    "        raise ValueError(\"Save directory path must be absolute\")\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    if not os.access(save_dir, os.W_OK):\n",
    "        raise ValueError(f\"Save directory {save_dir} is not writable\")\n",
    "\n",
    "# Function to get anime data from the provided URL\n",
    "async def get_anime_data(anime_url, download_type):\n",
    "    try:\n",
    "        anime_url = urlparse(anime_url)\n",
    "        episodes = []\n",
    "        page_i = 0\n",
    "        series_title = None\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            while True:\n",
    "                page_i += 1\n",
    "                anime_url = anime_url._replace(query=f'page={page_i}')\n",
    "                html_content = await fetch_html(session, anime_url.geturl())\n",
    "                if not html_content:\n",
    "                    break\n",
    "                soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "                if download_type == 'episodes':\n",
    "                    curr_episodes = soup.select(\"h3 > a[href*='/tv/episodeimages.php?']\")\n",
    "                else:  # download_type == 'whole_series'\n",
    "                    curr_episodes = soup.select(\"a[style='color:black;']\")\n",
    "\n",
    "                if not curr_episodes:\n",
    "                    break\n",
    "\n",
    "                for episode in curr_episodes:\n",
    "                    episode_title = episode.text.strip()\n",
    "                    episode_url = urljoin(anime_url.geturl(), episode['href'])\n",
    "                    episodes.append({'episodeTitle': episode_title, 'episodeUrl': episode_url})\n",
    "\n",
    "                if not series_title:\n",
    "                    series_title = soup.select_one(\"h1.post_title\").text.replace(': ', ' - ')\n",
    "\n",
    "        return {'seriesTitle': series_title, 'episodes': episodes}\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error fetching anime data from {anime_url}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Function to get TV show data from the provided URL\n",
    "async def get_tv_show_data(tv_show_url, download_type):\n",
    "    try:\n",
    "        tv_show_url = urlparse(tv_show_url)\n",
    "        episodes = []\n",
    "        page_i = 0\n",
    "        series_title = None\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            while True:\n",
    "                page_i += 1\n",
    "                tv_show_url = tv_show_url._replace(query=f'page={page_i}')\n",
    "                html_content = await fetch_html(session, tv_show_url.geturl())\n",
    "                if not html_content:\n",
    "                    break\n",
    "                soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "                if download_type == 'episodes':\n",
    "                    curr_episodes = soup.select(\"h3 > a[href*='/tv/episodeimages.php?']\")\n",
    "                else:  # download_type == 'whole_series'\n",
    "                    curr_episodes = soup.select(\"a[style='color:black;']\")\n",
    "\n",
    "                if not curr_episodes:\n",
    "                    break\n",
    "\n",
    "                for episode in curr_episodes:\n",
    "                    episode_title = episode.text.strip()\n",
    "                    episode_url = urljoin(tv_show_url.geturl(), episode['href'])\n",
    "                    episodes.append({'episodeTitle': episode_title, 'episodeUrl': episode_url})\n",
    "\n",
    "                if not series_title:\n",
    "                    series_title = soup.select_one(\"h1.post_title\").text.replace(': ', ' - ')\n",
    "\n",
    "        return {'seriesTitle': series_title, 'episodes': episodes}\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error fetching TV show data from {tv_show_url}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Function to get movie data from the provided URL\n",
    "async def get_movie_data(movie_url):\n",
    "    try:\n",
    "        movie_url = urlparse(movie_url)\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            html_content = await fetch_html(session, movie_url.geturl())\n",
    "            if html_content:\n",
    "                soup = BeautifulSoup(html_content, 'html.parser')\n",
    "                # Implement movie data scraping logic here\n",
    "                return {'movieData': 'Placeholder for movie data'}  # Replace with actual movie data structure\n",
    "            else:\n",
    "                logging.error(f\"Failed to fetch HTML content from {movie_url}\")\n",
    "                return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error fetching movie data from {movie_url}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Main function to handle argument parsing and invoke appropriate functions\n",
    "async def main(selection, download_type, url, save_dir, num_of_promises=75, skip_n_last_pages=2, disable_progress_bar=False):\n",
    "    try:\n",
    "        # Validate save directory\n",
    "        validate_save_dir(save_dir)\n",
    "\n",
    "        # Determine data retrieval function based on selection\n",
    "        if selection == 'Anime':\n",
    "            data = await get_anime_data(url, download_type)\n",
    "        elif selection == 'TV Series':\n",
    "            data = await get_tv_show_data(url, download_type)\n",
    "        elif selection == 'Movie':\n",
    "            data = await get_movie_data(url)\n",
    "        else:\n",
    "            logging.error(f\"Invalid selection: {selection}\")\n",
    "            return\n",
    "\n",
    "        if data:\n",
    "            # Downloading images or processing movie data\n",
    "            if 'episodes' in data:\n",
    "                await download_images_async([episode['episodeUrl'] for episode in data['episodes']], save_dir)\n",
    "            elif 'imageUrls' in data:\n",
    "                await download_images_async(data['imageUrls'], save_dir)\n",
    "            elif 'movieData' in data:\n",
    "                logging.info(f\"Movie data fetched: {data['movieData']}\")\n",
    "        else:\n",
    "            logging.error(f\"Failed to retrieve data from {url}\")\n",
    "    except ValueError as ve:\n",
    "        logging.error(f\"Error in save directory: {str(ve)}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in main function: {str(e)}\")\n",
    "\n",
    "# Define and display IPython widgets for user input\n",
    "selection_input = widgets.Dropdown(options=['Anime', 'TV Series', 'Movie'], description='Selection:')\n",
    "download_type_input = widgets.Dropdown(options=['episodes', 'whole_series'], description='Download Type:')\n",
    "url_input = widgets.Text(description='URL:')\n",
    "save_dir_input = widgets.Text(description='Save Directory:')\n",
    "num_of_promises_input = widgets.IntSlider(description='Number of Promises:', min=1, max=100, value=75)\n",
    "skip_n_last_pages_input = widgets.IntSlider(description='Skip Last Pages:', min=0, max=10, value=2)\n",
    "disable_progress_bar_input = widgets.Checkbox(description='Disable Progress Bar')\n",
    "\n",
    "def update_download_type(*args):\n",
    "    if selection_input.value == 'Movie':\n",
    "        download_type_input.disabled = True\n",
    "    else:\n",
    "        download_type_input.disabled = False\n",
    "\n",
    "selection_input.observe(update_download_type, 'value')\n",
    "\n",
    "display(selection_input, download_type_input, url_input, save_dir_input, num_of_promises_input, skip_n_last_pages_input, disable_progress_bar_input)\n",
    "\n",
    "# Run main function on button click\n",
    "button = widgets.Button(description='Run Scraper')\n",
    "\n",
    "async def on_button_clicked(b):\n",
    "    await main(selection_input.value, download_type_input.value, url_input.value, save_dir_input.value, num_of_promises_input.value, skip_n_last_pages_input.value, disable_progress_bar_input.value)\n",
    "\n",
    "button.on_click(on_button_clicked)\n",
    "display(button)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78401bcd-f3f9-4efb-9dbf-78cf54726a6e",
   "metadata": {},
   "source": [
    "# FiftyOneAI: Check for Duplicates\n",
    "\n",
    "To use Ngrok for tunneling in this script, you'll need an Ngrok authentication token. Follow these steps to obtain your Ngrok token:\n",
    "\n",
    "- Sign up for a free Ngrok account at Ngrok's website.\n",
    "- After signing up, log in to your Ngrok account.\n",
    "- Navigate to the Ngrok Auth page. Here, you'll find your authentication token.\n",
    "- Copy the token and paste it into the appropriate field in the script where prompted to integrate Ngrok functionality.\n",
    "- Save your changes and run the script. Ngrok will handle the tunneling through a randomly generated URL, allowing you to access your application remotely.\n",
    "\n",
    "Note: I'll add links later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f4fb38-4d86-4859-8aa7-68d58ef7b490",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import numpy as np\n",
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "from fiftyone import ViewField as F\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from pyngrok import ngrok\n",
    "\n",
    "# Function to start ngrok tunnel with provided token\n",
    "def start_ngrok(token):\n",
    "    try:\n",
    "        ngrok.set_auth_token(token)\n",
    "        public_url = ngrok.connect(5151)  # Replace 5151 with your app's port\n",
    "        print(f\"Ngrok Tunnel URL: {public_url}\")\n",
    "        return public_url\n",
    "    except Exception as e:\n",
    "        print(f\"Error starting ngrok tunnel: {e}\")\n",
    "        return None\n",
    "\n",
    "# Constants and configuration\n",
    "similarity_threshold = 0.985  # Adjust as needed\n",
    "model_name = \"clip-vit-base32-torch\"  # Model for embeddings\n",
    "supported_types = (\".png\", \".jpg\", \".jpeg\")  # Supported image types\n",
    "images_folder = \"/path/to/your/images/folder\"  # Replace with your actual path\n",
    "project_subfolder = \"fiftyone_project\"\n",
    "\n",
    "# Step 1: Check dependencies and set up environment\n",
    "if \"step1_installed_flag\" not in globals():\n",
    "    raise Exception(\"Please run step 1 first!\")  # Ensure dependencies are set up\n",
    "\n",
    "# Function to analyze dataset and remove duplicates\n",
    "def analyze_and_remove_duplicates(images_folder, project_subfolder, ngrok_token):\n",
    "    try:\n",
    "        os.chdir(images_folder)  # Change directory to your images folder\n",
    "        img_count = len(os.listdir(images_folder))\n",
    "        \n",
    "        if img_count == 0:\n",
    "            print(f\"üí• Error: No images found in {images_folder}\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\nüíø Analyzing dataset...\\n\")\n",
    "        dataset = fo.Dataset.from_dir(images_folder, dataset_type=fo.types.ImageDirectory)\n",
    "\n",
    "        # Step 2: Compute embeddings for images\n",
    "        model = foz.load_zoo_model(model_name)\n",
    "        embeddings = dataset.compute_embeddings(model)\n",
    "\n",
    "        batch_size = min(250, img_count)\n",
    "        batch_embeddings = np.array_split(embeddings, batch_size)\n",
    "        similarity_matrices = []\n",
    "        max_size_x = max(array.shape[0] for array in batch_embeddings)\n",
    "        max_size_y = max(array.shape[1] for array in batch_embeddings)\n",
    "\n",
    "        for i, batch_embedding in enumerate(batch_embeddings):\n",
    "            similarity = cosine_similarity(batch_embedding)\n",
    "            padded_array = np.zeros((max_size_x, max_size_y))\n",
    "            padded_array[0:similarity.shape[0], 0:similarity.shape[1]] = similarity\n",
    "            similarity_matrices.append(padded_array)\n",
    "\n",
    "        similarity_matrix = np.concatenate(similarity_matrices, axis=0)\n",
    "        similarity_matrix = similarity_matrix[0:embeddings.shape[0], 0:embeddings.shape[0]]\n",
    "\n",
    "        similarity_matrix = cosine_similarity(embeddings)\n",
    "        similarity_matrix -= np.identity(len(similarity_matrix))\n",
    "\n",
    "        # Step 3: Calculate similarity matrix and mark duplicates\n",
    "        dataset.match(F(\"max_similarity\") > similarity_threshold)\n",
    "        dataset.tags = [\"delete\", \"has_duplicates\"]\n",
    "\n",
    "        id_map = [s.id for s in dataset.select_fields([\"id\"])]\n",
    "        samples_to_remove = set()\n",
    "        samples_to_keep = set()\n",
    "\n",
    "        for idx, sample in enumerate(dataset):\n",
    "            if sample.id not in samples_to_remove:\n",
    "                # Keep the first instance of two duplicates\n",
    "                samples_to_keep.add(sample.id)\n",
    "\n",
    "                dup_idxs = np.where(similarity_matrix[idx] > similarity_threshold)[0]\n",
    "                for dup in dup_idxs:\n",
    "                    # Remove all other duplicates\n",
    "                    samples_to_remove.add(id_map[dup])\n",
    "\n",
    "                if len(dup_idxs) > 0:\n",
    "                    sample.tags.append(\"has_duplicates\")\n",
    "                    sample.save()\n",
    "            else:\n",
    "                sample.tags.append(\"delete\")\n",
    "                sample.save()\n",
    "\n",
    "        # Step 4: Launch FiftyOne app with Ngrok tunnel\n",
    "        ngrok_url = start_ngrok(ngrok_token)\n",
    "        if ngrok_url:\n",
    "            try:\n",
    "                # Launch the FiftyOne app with the Ngrok URL\n",
    "                fo.launch_app(dataset, port=5151, url=ngrok_url)\n",
    "\n",
    "                # Wait for user input to save changes\n",
    "                input(\"‚≠ï When you're done, enter something here to save your changes: \")\n",
    "\n",
    "                # Step 5: Remove marked samples and save changes\n",
    "                marked = [s for s in dataset if \"delete\" in s.tags]\n",
    "                dataset.remove_samples(marked)\n",
    "                dataset.export(export_dir=os.path.join(images_folder, project_subfolder), dataset_type=fo.types.ImageDirectory)\n",
    "\n",
    "                # Clean up temporary folders\n",
    "                temp_suffix = \"_temp\"\n",
    "                os.rename(images_folder, images_folder + temp_suffix)\n",
    "                os.rename(images_folder + temp_suffix + \"/\" + project_subfolder, images_folder)\n",
    "                os.rmdir(images_folder + temp_suffix)\n",
    "\n",
    "                print(f\"\\n‚úÖ Removed {len(marked)} images from dataset.\")\n",
    "                print(f\"You now have {len(os.listdir(images_folder))} images remaining.\")\n",
    "\n",
    "            finally:\n",
    "                # Disconnect Ngrok tunnel when done\n",
    "                ngrok.disconnect(ngrok_url)\n",
    "                print(\"Ngrok tunnel disconnected\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in analyzing and removing duplicates: {e}\")\n",
    "\n",
    "# Ngrok token input widget\n",
    "ngrok_token_input = widgets.Text(description='Ngrok Auth Token:', placeholder='Enter your ngrok auth token')\n",
    "\n",
    "# Function to handle ngrok token input\n",
    "def on_ngrok_token_submit(b):\n",
    "    clear_output()\n",
    "    display(ngrok_token_input)\n",
    "    analyze_and_remove_duplicates(images_folder, project_subfolder, ngrok_token_input.value)\n",
    "\n",
    "# Display ngrok token input widget\n",
    "display(ngrok_token_input)\n",
    "\n",
    "# Button to submit ngrok token\n",
    "submit_button = widgets.Button(description='Submit')\n",
    "submit_button.on_click(on_ngrok_token_submit)\n",
    "display(submit_button)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5463b5a3-cb54-4da3-a915-86e9eed6f86a",
   "metadata": {},
   "source": [
    "# Tagging Dataset (Under Construction - WIP xD need more time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07c5158-f321-47f5-a0c3-0970684b1ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Function to handle tagging based on user inputs\n",
    "def tag_images(method, tag_threshold, blacklist_tags, caption_min, caption_max, selected_tagger):\n",
    "    if method == \"Anime tags\":\n",
    "        if \"step4a_installed_flag\" not in globals():\n",
    "            print(\"\\nüè≠ Installing dependencies for Anime tags...\\n\")\n",
    "            !pip install accelerate==0.15.0 diffusers[torch]==0.10.2 einops==0.6.0 tensorflow transformers safetensors huggingface-hub torchvision albumentations jax==0.4.23 jaxlib==0.4.23\n",
    "            if not get_ipython().__dict__['user_ns']['_exit_code']:\n",
    "                clear_output()\n",
    "                step4a_installed_flag = True\n",
    "            else:\n",
    "                print(\"‚ùå Error installing dependencies, trying to continue anyway...\")\n",
    "\n",
    "        print(\"\\nüö∂‚Äç‚ôÇÔ∏è Launching program for Anime tags...\\n\")\n",
    "\n",
    "        # Adjust paths and commands as needed\n",
    "        kohya = \"/content/kohya-trainer\"\n",
    "        os.environ['PYTHONPATH'] = kohya\n",
    "        !python {kohya}/finetune/tag_images_by_wd14_tagger.py \\\n",
    "            {images_folder} \\\n",
    "            --repo_id={selected_tagger} \\\n",
    "            --model_dir={root_dir} \\\n",
    "            --thresh={tag_threshold} \\\n",
    "            --batch_size=8 \\\n",
    "            --caption_extension=.txt \\\n",
    "            --force_download\n",
    "\n",
    "        if not get_ipython().__dict__['user_ns']['_exit_code']:\n",
    "            print(\"Removing underscores and blacklist...\")\n",
    "            blacklisted_tags = [t.strip() for t in blacklist_tags.split(\",\")]\n",
    "            from collections import Counter\n",
    "            top_tags = Counter()\n",
    "            for txt in [f for f in os.listdir(images_folder) if f.lower().endswith(\".txt\")]:\n",
    "                with open(os.path.join(images_folder, txt), 'r') as f:\n",
    "                    tags = [t.strip() for t in f.read().split(\",\")]\n",
    "                    tags = [t.replace(\"_\", \" \") if len(t) > 3 else t for t in tags]\n",
    "                    tags = [t for t in tags if t not in blacklisted_tags]\n",
    "                top_tags.update(tags)\n",
    "                with open(os.path.join(images_folder, txt), 'w') as f:\n",
    "                    f.write(\", \".join(tags))\n",
    "\n",
    "            os.environ['PYTHONPATH'] = '/env/python'\n",
    "            clear_output()\n",
    "            print(f\"üìä Tagging complete. Here are the top 50 tags in your dataset:\")\n",
    "            print(\"\\n\".join(f\"{k} ({v})\" for k, v in top_tags.most_common(50)))\n",
    "\n",
    "    elif method == \"Photo captions\":\n",
    "        if \"step4b_installed_flag\" not in globals():\n",
    "            print(\"\\nüè≠ Installing dependencies for Photo captions...\\n\")\n",
    "            !pip install timm==0.6.12 fairscale==0.4.13 transformers==4.26.0 requests==2.28.2 accelerate==0.15.0 diffusers[torch]==0.10.2 einops==0.6.0 safetensors==0.2.6 jax==0.4.23 jaxlib==0.4.23\n",
    "            if not get_ipython().__dict__['user_ns']['_exit_code']:\n",
    "                clear_output()\n",
    "                step4b_installed_flag = True\n",
    "            else:\n",
    "                print(\"‚ùå Error installing dependencies, trying to continue anyway...\")\n",
    "\n",
    "        print(\"\\nüö∂‚Äç‚ôÇÔ∏è Launching program for Photo captions...\\n\")\n",
    "\n",
    "        # Adjust paths and commands as needed\n",
    "        kohya = \"/content/kohya-trainer\"\n",
    "        os.environ['PYTHONPATH'] = kohya\n",
    "        !python {kohya}/finetune/make_captions.py \\\n",
    "            {images_folder} \\\n",
    "            --beam_search \\\n",
    "            --max_data_loader_n_workers=2 \\\n",
    "            --batch_size=8 \\\n",
    "            --min_length={caption_min} \\\n",
    "            --max_length={caption_max} \\\n",
    "            --caption_extension=.txt\n",
    "\n",
    "        if not get_ipython().__dict__['user_ns']['_exit_code']:\n",
    "            import random\n",
    "            captions = [f for f in os.listdir(images_folder) if f.lower().endswith(\".txt\")]\n",
    "            sample = []\n",
    "            for txt in random.sample(captions, min(10, len(captions))):\n",
    "                with open(os.path.join(images_folder, txt), 'r') as f:\n",
    "                    sample.append(f.read())\n",
    "\n",
    "            os.chdir(root_dir)\n",
    "            os.environ['PYTHONPATH'] = '/env/python'\n",
    "            clear_output()\n",
    "            print(f\"üìä Captioning complete. Here are {len(sample)} example captions from your dataset:\")\n",
    "            print(\"\".join(sample))\n",
    "\n",
    "# IPython widgets for user interaction\n",
    "method_dropdown = widgets.Dropdown(\n",
    "    options=[\"Anime tags\", \"Photo captions\"],\n",
    "    description='Tagging Method:'\n",
    ")\n",
    "\n",
    "tag_threshold_slider = widgets.FloatSlider(\n",
    "    value=0.35,\n",
    "    min=0.0,\n",
    "    max=1.0,\n",
    "    step=0.01,\n",
    "    description='Tag Threshold:'\n",
    ")\n",
    "\n",
    "blacklist_tags_text = widgets.Text(\n",
    "    placeholder='Enter comma-separated tags',\n",
    "    description='Blacklist Tags:'\n",
    ")\n",
    "\n",
    "caption_min_text = widgets.IntText(\n",
    "    value=10,\n",
    "    description='Caption Min Length:'\n",
    ")\n",
    "\n",
    "caption_max_text = widgets.IntText(\n",
    "    value=75,\n",
    "    description='Caption Max Length:'\n",
    ")\n",
    "\n",
    "tagger_dropdown = widgets.Dropdown(\n",
    "    options=[\"SmilingWolf/wd-v1-4-swinv2-tagger-v2\", \"Other taggers...\"],\n",
    "    description='Tagger Selection:'\n",
    ")\n",
    "\n",
    "run_button = widgets.Button(\n",
    "    description='Run Tagging',\n",
    "    button_style='success',\n",
    "    tooltip='Click to run tagging process'\n",
    ")\n",
    "\n",
    "def on_run_button_clicked(b):\n",
    "    clear_output()\n",
    "    display(method_dropdown, tag_threshold_slider, blacklist_tags_text, caption_min_text, caption_max_text, tagger_dropdown, run_button)\n",
    "    tag_images(method_dropdown.value, tag_threshold_slider.value, blacklist_tags_text.value, caption_min_text.value, caption_max_text.value, tagger_dropdown.value)\n",
    "\n",
    "run_button.on_click(on_run_button_clicked)\n",
    "\n",
    "# Display widgets\n",
    "display(method_dropdown, tag_threshold_slider, blacklist_tags_text, caption_min_text, caption_max_text, tagger_dropdown, run_button)\n",
    "\n",
    "# Be aware this isn't finished, and is LOVINGLY BASED ON HOLOSTRAWBERRY'S colab!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
